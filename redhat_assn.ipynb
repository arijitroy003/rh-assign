{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This python code can run on a Lambda which runs and pushes data into a DataLake/s3 at 't' interval\n",
    "\n",
    "# For Local -> pushes into a folder\n",
    "\n",
    "req_data = requests.get('https://fakerapi.it/api/v1/products?_quantity=1&_taxes=12&_categories_type=uuid')\n",
    "data_json = req_data.json()\n",
    "# data_json\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "with open('raw_data_lake/file_'+timestamp+'.json', 'w') as f:\n",
    "    json.dump(data_json, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file_2024-02-29_19:35:01.json',\n",
       " 'file_2024-02-29_19:14:06.json',\n",
       " 'file_2024-02-29_19:14:05.json',\n",
       " 'file_2024-02-29_19:34:57.json',\n",
       " 'file_2024-02-29_19:34:59.json',\n",
       " 'file_2024-02-29_21:18:46.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir('raw_data_lake')\n",
    "files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬───────┬───────┬────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ status  │ code  │ total │                                            data                                            │\n",
       "│ varchar │ int64 │ int64 │ struct(id bigint, \"name\" varchar, description varchar, ean varchar, upc varchar, image v…  │\n",
       "├─────────┼───────┼───────┼────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ OK      │   200 │     1 │ [{'id': 1, 'name': Aut earum sequi nam aut., 'description': Assumenda incidunt iste porr…  │\n",
       "└─────────┴───────┴───────┴────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import duckdb\n",
    "# cursor = duckdb.connect()\n",
    "# duckdb.sql(\"SELECT * FROM read_json_auto('data.json')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data into BRONZE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  status  code  total                                               data\n",
      "0     OK   200      1  [{'id': 1, 'name': 'Ex quia non est.', 'descri...\n",
      "1     OK   200      1  [{'id': 1, 'name': 'Unde ab sit est minus.', '...\n",
      "2     OK   200      1  [{'id': 1, 'name': 'Quis aliquam voluptatem eo...\n",
      "3     OK   200      1  [{'id': 1, 'name': 'Totam illum quod quis mini...\n",
      "4     OK   200      1  [{'id': 1, 'name': 'Eos animi dolor aperiam ip...\n",
      "5     OK   200      1  [{'id': 1, 'name': 'Ipsa laborum dolore evenie...\n"
     ]
    }
   ],
   "source": [
    "#loads data into duckdb -> Local Solution\n",
    "\n",
    "# We can load data into any database service like databricks and \n",
    "# load it into delta tables (Solves our requirements of getting data in any point of time)\n",
    "\n",
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "def read_json_files(directory):\n",
    "    dfs = []\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            \n",
    "            # Read the JSON file into a DataFrame using DuckDB\n",
    "            con = duckdb.connect(':memory:')\n",
    "            con.execute(f'CREATE TABLE temp AS SELECT * FROM read_json_auto(\\'{file_path}\\')')\n",
    "            df = con.execute('SELECT * FROM temp').fetchdf()\n",
    "            con.close()\n",
    "            \n",
    "            dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "directory = './raw_data_lake'\n",
    "\n",
    "# Read JSON files into a DataFrame\n",
    "df = read_json_files(directory)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working data into SILVER Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark dataframes to run large transformations in distributed systems\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# We check for arrays in our data column field and create new rows for each element\n",
    "# Explode the array column to create multiple rows\n",
    "exploded_df = spark_df.select(col(\"id\"), explode(\"data\").alias(\"json_string\"))\n",
    "\n",
    "#This parses our json strings into a queryable format\n",
    "ready_to_write_into_table_df = exploded_df.withColumn('json_data',F.from_json(df.value,MapType(StringType(),StringType())))\n",
    "\n",
    "# Now we can build out our transformations/normalization/deduplication/features on top on this data\n",
    "\n",
    "#Requirement Gathering and Pre- Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Fault Tolerance and Setup Unit Tests for our Data as per requirements\n",
    "- python udfs / pytest\n",
    "- sqlmesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Orchestration->\n",
    "- We can use Airflow/ Overwatch / Tools like Azure Data Factory\n",
    "- Audit Logs management with services like Splunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Governance and Isolation of Catalogs/Lineage\n",
    "- Unity Catalog / Microsoft Purview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta tables need maintenance as well since its a file system type database\n",
    "\n",
    "- VACUUM is used to clean up unused and stale data files that are taking up unnecessary storage space. Removing these files can help reduce storage costs\n",
    "\n",
    "    > When you run VACUUM on a Delta table it removes the following files from the underlying file system:\n",
    "    > - Any data files that are not maintained by Delta Lake\n",
    "    > - Removes stale data files (files that are no longer referenced by a Delta table) and are older than 7 days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OPTIMIZE/ Z-Ordering -> Optionally optimize a subset of data or collocate data by column. If you do not specify collocation and the table is not defined with liquid clustering, bin-packing optimization is performed.\n",
    "\n",
    "    > Bin-packing aims to produce evenly-balanced data files with respect to their size on disk, but not necessarily number of tuples per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
